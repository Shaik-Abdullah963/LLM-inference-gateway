# LLM Inference Gateway Configuration

# Server Settings
HOST=0.0.0.0
PORT=8000

# Model Settings
MODEL_NAME=gpt2
MODEL_CACHE_DIR=./models
MAX_LENGTH=512
TEMPERATURE=0.7
TOP_P=0.9

# Performance Settings
MAX_BATCH_SIZE=8
DEVICE=cpu

# Observability
LOG_LEVEL=INFO
METRICS_ENABLED=true
METRICS_PORT=9090
