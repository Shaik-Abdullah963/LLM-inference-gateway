# LLM-inference-gateway
An evolving LLM inference gateway built from the ground up. It starts with a simple, CPU-friendly foundation and will gradually grow into a high-performance inference server—with batching, KV-cache reuse, streaming responses, hot caching, observability, and real benchmarking—similar to systems like vLLM, TGI, and Triton.
